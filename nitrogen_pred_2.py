# -*- coding: utf-8 -*-
"""nitrogen_pred_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PrJuieH-zBm2k-fgGGJM_zCEbktajc5i
"""

# !pip install lazypredict
#import lazypredict
#from lazypredict.Supervised import LazyRegressor # importáljuk be a Regressort! ezt majd később használjuk, de importáljuk itt, mert restartot igényel

import plotly.express as px
import numpy as np
import pandas as pd
import plotly.graph_objects as go

#Nitrogén predikció
!rm /content/all.csv
!wget "https://github.com/sipocz/messer/raw/main/all.csv"
!head /content/all.csv



import numpy as np
import pandas as pd

_PCVERSION_=False
_GITHUB_ORIG=True

if _PCVERSION_:
    basedir="C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/nitrogen"
else:

    if _GITHUB_ORIG:
        basedir="/content/"
    else:
        from google.colab import drive
        drive.mount('/content/drive',force_remount=True)
        basedir="/content/drive/My Drive/001_AI/messer/nitrogen/"
print(basedir)

df=pd.read_csv(basedir+"all.csv")
df.head()

df["timeStamp"]=pd.to_datetime(df["timeStamp"],format="%Y.%m.%d %H:%M:%S")   # konvertáljuk az időbélyeget


d=df[["timeStamp","PSA_M"]]
d.set_index("timeStamp",inplace=True)

d.head()

agg_10m = d.groupby([pd.Grouper(freq='D')])
resample_mean =  d.resample("60min").agg({'PSA_M': 'mean'})  # újramintázzuk a jeleket, pl órás átlagokkal számoljunk
resample_max =  d.resample("60min").agg({'PSA_M': 'max'})    # újramintázzuk a jeleket, pl órás maximumok
resample_min =  d.resample("60min").agg({'PSA_M': 'min'})    # újramintázzuk a jeleket, pl órás minimumok
o_df=pd.DataFrame({"Max":resample_max.PSA_M,"Min":resample_min.PSA_M, "Mean":resample_mean.PSA_M})

#vizualizáció
o_df.head()

import plotly.graph_objects as go


#fig = px.scatter(o_df,x=o_df.index, y=o_df.Mean, title='Nitrogén tömegáram [Órás átlagok]',range_y=(0,1500),color=["rgb(100,0,0,0.5)"])
fig = go.Figure(go.Scatter(x=o_df.index, y=o_df.Mean,name="Mean"), layout_yaxis_range=[0,1100])

fig.update_layout   (  
                    width=1900,
                    
                    title_text="Órás átlagértékek",
                    )

fig.show()

import plotly.graph_objects as go


range1=0
range2=-1

fig = go.Figure([go.Scatter(x=o_df.index[range1:range2], y=o_df.Min,name="Min"),go.Scatter(x=o_df.index[range1:range2], y=o_df.Max, name="Max")])
fig.update_xaxes(
    dtick="M1",
    tickformat="'%d %B (%a)")
fig.update_yaxes(
    dtick="500")
fig.update_layout(  width=1900,  
                    title_text="Min Max Values",
                    )

fig.update_layout(yaxis_range=[0,1300])
fig.show()

o_df.notna()

print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
o_df.dropna(inplace=True)
print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )

# Augmented Dickey-Fuller Test
from statsmodels.tsa.stattools import adfuller

def ADF_test(timeseries, dataDesc):
    print(' {} ellenőrzése'.format(dataDesc))
    dftest = adfuller(timeseries.dropna(), autolag='AIC')
    print(dftest)
    print('Test statistic = {:.3f}'.format(dftest[0]))
    print('P-value = {:.3f}'.format(dftest[1]))
    print('Critical values :')
    for k, v in dftest[4].items():
        print('\t{}: {} - The data is {} stationary with {}% confidence'.format(k, v, 'not' if v<dftest[0] else '', 100-int(k[:-1])))
    print("--------------------------------------------")

ADF_test(o_df.Min,'Minimum értékek')
ADF_test(o_df.Max,'Maximum értékek')
ADF_test(o_df.Mean,'Átlag értékek ')

start_pos = 2386 #@param {type:"slider", min:1000, max:5000, step:1}

weeknum =  9#@param {type:"integer"}

"""## 24 órás frekvencia?"""

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure





plt.cla()
plt.close()

from statsmodels.tsa.seasonal import  seasonal_decompose

import matplotlib as mpl
with mpl.rc_context():
    mpl.rc("figure", figsize=(16,8))
    
    analysis = seasonal_decompose(o_df["Mean"][start_pos:start_pos+24*7*weeknum].values,model="additive", freq=24)
    #print(analysis.seasonal)
    analysis.plot()
    plt.show()

"""## Heti fekvencia ? 24*7 """

start_pos = 3118 #@param {type:"slider", min:1000, max:5000, step:1}

weeknum =  12#@param {type:"integer"}

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure


from statsmodels.tsa.seasonal import  seasonal_decompose

import matplotlib as mpl

start_pos = 1702 #@param {type:"slider", min:1000, max:5000, step:1}

weeknum =  12#@param {type:"integer"}

import plotly.express as px
from plotly.subplots import make_subplots

data1 = seasonal_decompose(o_df["Max"][start_pos:start_pos+24*7*weeknum].values,model="additive", freq=24*7)
  

fig2 = make_subplots(rows=4, cols=1,shared_xaxes=True)
fig2.add_trace(
    go.Scatter(x=o_df.index[start_pos:start_pos+24*7*weeknum], y=data1.resid,name="Residual" ),

    row=1, col=1
)
fig2.add_trace(
    go.Scatter( x=o_df.index[start_pos:start_pos+24*7*weeknum], y=data1.trend,name="Trend"),

    row=2, col=1
)
fig2.add_trace(
    go.Scatter( x=o_df.index[start_pos:start_pos+24*7*weeknum], y=data1.seasonal,name="Seasonal"),

    row=3, col=1
)

fig2.add_trace(
    go.Scatter( x=o_df.index[start_pos:start_pos+24*7*weeknum], y=data1.observed,name="Observed"),

    row=4, col=1
)



fig2.update_layout(
    font_family="Courier New",
    font_color="blue",
    title_font_family="Times New Roman",
    title_font_color="red",
  
)

fig2.show()


print(o_df.index[start_pos])

from statsmodels.tsa.stattools import pacf, acf 
oa,ob=pacf(o_df.Max[:25000], nlags=600, alpha=0.1,)

import plotly.express as px

xl=list(range(len(oa)))
fig = px.line(oa,x=xl, y=oa, title='Nitrogén tömegáram',range_y=(-1,1),)

fig.add_trace(
    go.Scatter(x=xl,
               y=ob[:,1],
               )
    

)
fig.add_trace(
    go.Scatter(x=xl,
               y=ob[:,0],
               )
    

)


fig.show()

"""The null hypothesis of the test is that there is no serial correlation in the residuals. The Durbin-Watson test statistic is defined as:

∑t=2T((et−et−1)2)/∑t=1Te2t
The test statistic is approximately equal to 2*(1-r) where r is the sample autocorrelation of the residuals. Thus, for r == 0, indicating no serial correlation, the test statistic equals 2. This statistic will always be between 0 and 4. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4, the more evidence for negative serial correlation.
"""

from statsmodels.stats.stattools import durbin_watson
from statsmodels.tsa.stattools import adfuller
o1=durbin_watson(o_df.Min)
print(f"Durbin_Wattson elemzés eredménye: {o1}")
o2=adfuller(o_df.Mean)
print(f"ad_Fuller elemzés eredménye: {o2[1]},  nulla (0) ha nincs tendencia")

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf

fig, ax = plt.subplots(figsize=(16,8))


plot_pacf(o_df.Max[:], lags=500, zero=False, ax=ax)
plt.xlabel("Measurement points back in time - relatively")

plt.show()
print("---")

import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
dta = o_df.Max[:5000]
dta.index = o_df.index[:5000]


sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=24*7*7)
plt.show()

#statsmodels.graphics.tsa
o_df

import plotly.express as px
fig = px.histogram(o_df, x=o_df["Max"],log_y=False)
fig.show()

#Outlier detektálás

from sklearn.cluster import DBSCAN
import numpy as np
X = o_df
print(type(X))
clustering = DBSCAN(eps=100, min_samples=1).fit(X)
print(clustering.labels_)

"""### Nézzünk egy histogramot az eloszlásról"""

import plotly.express as px
df=pd.DataFrame(clustering.labels_)

fig = px.histogram(df,x=df[0])
fig.show()

"""### Betyárúl néz ki !!"""

print (o_df.describe())

amax=df.max
o=[index for (index,i) in enumerate(clustering.labels_) if i>0]
print(clustering.labels_[o])
print(o_df.Max[o])

### Találtunk egypár outliert szedjük ki a dataframe-ből

print("Ezek lesznek az Outlierek")
print(o_df.Max[o])
print("\n\nAz eredeti Dataframe:\n",o_df.describe())
o_df_clear=o_df.drop(o_df.index[o])
print("\n\nA végleges Dataframe:\n",o_df_clear.describe())

"""### Sikerült kiszedni, nézzünk egy hisztogramot"""

df=pd.DataFrame(clustering.labels_)

fig = px.histogram(o_df_clear,x="Max",opacity=0.8,nbins=400)
fig.show()

start_pos = 1994 #@param {type:"slider", min:1000, max:5000, step:1}

weeknum =  12#@param {type:"integer"}

import plotly.express as px
from plotly.subplots import make_subplots


data1 = seasonal_decompose(o_df_clear["Max"][start_pos:start_pos+24*7*weeknum].values,model="additive", freq=24*7)
  

fig2 = make_subplots(rows=4, cols=1,shared_xaxes=True)
fig2.add_trace(
    go.Scatter(x=o_df_clear.index[start_pos:start_pos+24*7*weeknum], y=data1.resid,name="Residual" ),

    row=1, col=1
)
fig2.add_trace(
    go.Scatter( x=o_df_clear.index[start_pos:start_pos+24*7*weeknum], y=data1.trend,name="Trend"),

    row=2, col=1
)
fig2.add_trace(
    go.Scatter( x=o_df_clear.index[start_pos:start_pos+24*7*weeknum], y=data1.seasonal,name="Seasonal"),

    row=3, col=1
)

fig2.add_trace(
    go.Scatter( x=o_df_clear.index[start_pos:start_pos+24*7*weeknum], y=data1.observed,name="Observed"),

    row=4, col=1
)



fig2.update_layout(
    font_family="Courier New",
    font_color="blue",
    title_font_family="Times New Roman",
    title_font_color="red",
  
)

fig2.show()


print(o_df_clear.index[start_pos])

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(o_df_clear, test_size=0.2, shuffle=False)

def create_train_data(df,selector="Mean", sample=10,test_size=0.2,printout=False):
    '''
    df dataframe selector által meghatározott oszlopából 10 sample méretű idősor tanulóset-et készít
    
    '''
    sample+=1
    X_train=[]
    y_train=[]
    X_test=[]
    y_test=[]
    
    train_data, test_data = train_test_split(df, test_size=test_size, shuffle=False)
    
    for t in range(len(list(train_data[selector]))-sample):
        a=list(train_data[selector][t:t+sample])
    
        #print(a)
        X_train.append(a[0:sample-1])
        #print(a)
        y_train.append([a[-1]])

    for t in range(len(list(test_data[selector]))-sample):
        a=list(test_data[selector][t:t+sample])
        X_test.append(a[0:sample-1])
        y_test.append([a[-1]])

    if printout:
        for i in range(10):
            print(X_train[i], y_train[i])

    return(X_train,y_train,X_test,y_test)

"""#készítsünk egy tanuló szetet !"""

sample=10
X_train=[]
y_train=[]
X_test=[]
y_test=[]
X_train,y_train,X_test,y_test=create_train_data(o_df_clear,selector="Mean",sample=sample,test_size=0.2,printout=True)

"""### Ez jónak tűnik!!!
Nézzük meg mivel érdemes tanítani!
"""

#!!pip install lazypredict  # Lazypredict egy gyors tesztlehetőséget ad, és semmit sem kell csinálni!!!!



#regressor = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)
#rego=regressor.fit(pd.DataFrame(X_train),pd.DataFrame(X_test),pd.DataFrame(y_train),pd.DataFrame(y_test))

#print(rego[0][0:-10])

"""### MLP regressor nyert, ezt egy neuralis network alapú cucc, ne ezzel próbáljuk először.
###Legyen pl.: GradientBoostingRegressor

"""

# from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
regressor=GradientBoostingRegressor(learning_rate=0.5,n_estimators=3000,verbose=1)
regressor.fit(X_train,y_train)
prediction=regressor.predict(X_test)

# gyártunk egy X tengelyt a plotly-nak
x_=[i for i in range(-1*sample,len(prediction))]

import plotly.express as px
from plotly.subplots import make_subplots

  

fig2 = make_subplots(rows=1, cols=1,shared_xaxes=True)


fig2.add_trace(
    go.Scatter(x=x_, y=pd.DataFrame(y_test)[0],name="Testvalues" ),

    row=1, col=1
)

fig2.add_trace(
    go.Scatter(x=x_, y=prediction,name="Prediction",fill="none",opacity=0.5),

    row=1, col=1
    
)



fig2.show()

"""#Mi lenne, ha a minimumot és a maximumot predikálnánk az átlagnak közötte kell lenni!"""

from sklearn.model_selection import train_test_split
sample=2
testsize=0.05

Mean_X_train,Mean_y_train,Mean_X_test,Mean_y_test=create_train_data(o_df_clear,selector="Mean", sample=sample,test_size=testsize,printout=True)

Min_X_train,Min_y_train,Min_X_test,Min_y_test=create_train_data(o_df_clear,selector="Min",sample=sample,test_size=testsize,printout=True)

Max_X_train,Max_y_train,Max_X_test,Max_y_test=create_train_data(o_df_clear,selector="Max",sample=sample,test_size=testsize,printout=True)

# from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
maxregressor=GradientBoostingRegressor(learning_rate=0.5,n_estimators=4500,verbose=1)
maxregressor.fit(Max_X_train,Max_y_train)
Max_prediction=maxregressor.predict(Max_X_test)

# from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
minregressor=GradientBoostingRegressor(learning_rate=0.5,n_estimators=4500,verbose=1)
minregressor.fit(Min_X_train,Min_y_train)
Min_prediction=minregressor.predict(Min_X_test)

minregressor.predict(Min_X_test[0:10])

Min_X_test[0:10]

Max_x_=[i for i in range(1*(sample),len(Max_prediction))]
Min_x_=[i for i in range(1*(sample),len(Min_prediction))]
x_=[i for i in range(-0*(sample),len(Mean_y_test))]

import plotly.express as px
from plotly.subplots import make_subplots

  

fig2 = make_subplots(rows=1, cols=1,shared_xaxes=True)


'''


'''
fig2.add_trace(
    go.Scatter(x=x_, y=pd.DataFrame(Mean_y_test)[0],name="Testvalues" ),

    row=1, col=1
)

fig2.add_trace(
    go.Scatter(x=Max_x_, y=Max_prediction,name="Maxvalues", line=dict(color='rgba(200,0,0,0.1)'),showlegend=False ),

    row=1, col=1
)

fig2.add_trace(
    go.Scatter(x=Min_x_,
               y=Min_prediction,
               name="Minvalues",
               line=dict(color='rgba(200,0,0,0.1)'),
               fillcolor='rgba(0,100,80,0.2)',
               showlegend=False,
               fill='tonexty'
               ),

    row=1, col=1

)


fig2.show()

"""# Prophet tesz"""

o_df.head()

pro_df=pd.DataFrame(data={"ds":o_df.index,"y":o_df.Mean})

pro_df.head()

# van az adathalmazban adathiba kivesszük!
for i in pro_df.index:
    if pro_df.loc[i,"y"]>1600:
        #print(i,df_korr.loc[i,"PSA_M"])
        pro_df.drop(index=i,inplace=True)

from fbprophet import Prophet
m = Prophet(yearly_seasonality=True)
m.fit(pro_df[:-5000])
forecast = m.predict(pro_df[-5000:-4900],)

profet_graf=m.plot(forecast,figsize=(25,10),)
m.plot_components(forecast);

forecast.head()

forecast.head()

# Python
from fbprophet.plot import plot_plotly, plot_components_plotly
import plotly.graph_objects as go
from plotly import tools
import plotly.express as px
import pandas as pd

fbfig=plot_plotly(m, forecast)


fbfig.update_layout(
    autosize=False,
    width=1600,
    height=900,
    
    )

fbfig.add_trace(
    go.Scatter(x=pro_df[-5000:-4900].ds, y=pro_df[-5000:-4900].y,
            name="Test data",
            showlegend=True,
            mode="markers",
            marker=dict(color='rgba(200,0,0,0.9)', 
            symbol="diamond")   ),
 
   
    


)


fbfig.show()

"""#**Ide még lehetne egy kis prophet tesztet beiktatni, vajon mit mutat a pillanatnyi adatokra?**

---

---
"""

import plotly.express as px
import numpy as np
import pandas as pd
import plotly.graph_objects as go

df=pd.read_csv("/content/all.csv")
print(df.info())
print(df)

df_fb=pd.DataFrame(data={"ds":df.timeStamp,"y":df.PSA_M})
df_fb.head()

print(f"{df_fb['y'].isnull().sum()} db NAN van a DF-ben" )

db=0
for i in df_fb.index:
    if df_fb.loc[i,"y"]>1500:
        #print(i,df_korr.loc[i,"PSA_M"])
        db+=1
        if db%100==0:
            print(f"{db}..", end="")
        df_fb.drop(index=i,inplace=True)

print(f"\nMűszaki határokon kívül eső {db}. db elem törölve!")

from fbprophet import Prophet
m = Prophet(yearly_seasonality=True)
m.fit(df_fb[:-5000])
forecast = m.predict(df_fb[-5000:-4900],)

profet_graf=m.plot(forecast,figsize=(25,10),)
m.plot_components(forecast);

# Python
from fbprophet.plot import plot_plotly, plot_components_plotly
import plotly.graph_objects as go
from plotly import tools
import plotly.express as px
import pandas as pd

fbfig=plot_plotly(m, forecast)


fbfig.update_layout(
    autosize=False,
    width=1600,
    height=900,
    
    )

fbfig.add_trace(
    go.Scatter(x=df_fb[-5000:-4900].ds, y=df_fb[-5000:-4900].y,
            name="Test data",
            showlegend=True,
            mode="markers",
            marker=dict(color='rgba(200,0,0,0.9)', 
            symbol="diamond")   ),
 
   
    


)


fbfig.show()

"""---

---

#Regresszió, korreláció keresés

Kérdés: Lehet e pótolni a hiányzó adatoka a többi adat ismeretében?
"""

# importáljunk, akkor ez a rész függetlenül is tud működni!
import plotly.express as px
import numpy as np
import pandas as pd
import plotly.graph_objects as go

# Beolvassuk ismét az adatsort

!rm /content/all.csv
!wget "https://github.com/sipocz/messer/raw/main/all.csv"
!head /content/all.csv

df_korr=pd.read_csv("/content/all.csv")

df_korr.head()

fig = px.histogram(df_korr,x="LIN_P",opacity=0.8,nbins=400)
fig.show()

#outlier detection

from sklearn.cluster import DBSCAN

X=list(df_korr.PSA_M)
X=np.reshape(X, (-1, 1))

dbscan=DBSCAN(eps=5.5, min_samples=60)

dbscan.fit(X)

labels=dbscan.labels_
print(labels)

outliers=[1 if i==-1 else 0  for i in labels]
inliers= [1 if i!=-1 else 0  for i in labels]
#print(outliers)
fig = px.scatter(df_korr,x=df_korr.index,y=df_korr["PSA_M"],opacity=0.8, title="PSA tömegáram [Nm3/h] nyers adatokkal ")
fig.show()
df_om=df_korr
## Kiszedjük azokat az elemeket, ahol műszaki határokat átlépve kaptunk adatokat
for i in df_om.index:
    if df_om.loc[i,"PSA_M"]>1500:
        #print(i,df_korr.loc[i,"PSA_M"])
        df_om.drop(index=i,inplace=True)

fig = px.scatter(df_korr,x=df_om.index,y=df_om["PSA_P"],opacity=0.8,title="PSA nyomásértékek [Bar], nyers adatok")

fig.show()

## Kiszedjük azokat az elemeket, ahol műszaki határokat átlépve kaptunk adatokat ha áll a berendezés nem lesz nyomása
for i in df_om.index:
    if df_om.loc[i,"PSA_P"]<1:
        #print(i,df_korr.loc[i,"PSA_M"])
        df_om.drop(index=i,inplace=True)

# nézzük mit csináltunk!!

fig = px.scatter(df_om,x=df_om.index,y=df_om["PSA_P"],opacity=0.8,title="PSA nyomásértékek [Bar], korrekció után")

fig.show()

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go



fig2 = make_subplots(rows=1, cols=1,shared_xaxes=True)
fig2.add_trace(
    go.Histogram(x=df_om["PSA_P"],name="OM",nbinsx=40,),

    row=1, col=1
)
fig2.add_trace(
    go.Histogram(x=df_korr["PSA_P"],name="O", nbinsx=40 ),

    row=1, col=1
)





fig2.update_layout(
    font_family="Courier New",
    font_color="blue",
    title_font_family="Times New Roman",
    title_font_color="red",
  
)

fig2.show()

print("Amivel foglalkoznunk kell:  ",df_om.columns)

fig = px.scatter(df_korr,x=df_om.index,y=df_om["PSA_Q"],opacity=0.8,title="A rendszerből kilépő Nitrogén maradó Oxigén koncentrációja [%]")

fig.show()

fig = px.scatter(df_om,x=df_om.index,y=df_om["LIN_V"],opacity=0.8, title="Folyékony Nitrogén tartály szintje [Nm3]")

fig.show()

fig = px.scatter(df_om,x=df_om.index,y=df_om["LIN_P"],opacity=0.5,title="Folyékony nitrogén tartály nyomása [Bar]")

fig.show()

#Nézzünk egy korrelációs mátrixot

corr_mat=df_om.corr()
print(corr_mat)

fig = go.Figure(data=go.Heatmap(
                    z=corr_mat,
                    x=corr_mat.columns,
                    y=corr_mat.columns,
                    colorscale=[[0,'white'], [1,'red']]
                   )
)

fig.update_layout(
    autosize=False,
    width=500,
    height=500,)
fig.show()

# Készítsünk egy scatter plotot!
from pandas.plotting import scatter_matrix

scatter_matrix(df_om, alpha = 0.2, figsize = (10, 10), diagonal = 'kde');

prediktorok=["LIN_V","LIN_P","PSA_P","PSA_Q"]
from sklearn.linear_model import Lasso
alpha=0.01
lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)
lassoreg.fit(df_om[prediktorok],df_om[["PSA_M"]])
y_pred = lassoreg.predict(df_om[prediktorok])

x_=[i for i in range(len(y_pred))]

import plotly.express as px
from plotly.subplots import make_subplots

  

fig2 = make_subplots(rows=2, cols=1,shared_xaxes=True,)


'''


'''
fig2.add_trace(
    go.Scatter(x=x_, y=y_pred,name="Predikált érték [Nm3/h]" ),

    row=1, col=1
)

fig2.add_trace(
    go.Scatter(x=x_, y=list(df_om["PSA_M"]),name="Mért érték [Nm3/h]", line=dict(color='rgba(200,0,0,0.2)') ,showlegend=True  ),

    row=1, col=1
)

fig2.add_trace(
    go.Scatter(x=x_, y=list(df_om["LIN_V"]),name="LIN tartály tartalma [Nm3]", line=dict(color='rgba(200,100,1000,0.7)') ,showlegend=True  ),

    row=2, col=1,
    
)



fig2.update_layout(
    autosize=False,
    width=1600,
    height=900,
    )

fig2.update_layout(title="A negatív predikált értékek nyilvánvalóan fizikai képtelenségek, azonban érdemes lenne vizsgálni az okokat.")



fig2.show()

"""A negatív predikált értékek nyilvánvalóan fizikai képtelenségek, azonban érdemes lenne vizsgálni az okokat. A LIN tartály folyékony nitrogénnel való töltése formálisan ellentétes irányú áramlásnak is felfogható. A LIN tartály szintváltozása a 2. grafikonon!!

---

---

# Kezdjük újra, TensorFlow!!
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <h1>TENSORFLOW </h1>

def grafikon(y_pred,y_test):

    x_=[i for i in range(len(y_pred))]

    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    fig0 = make_subplots(rows=1, cols=1,)
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_pred), name="Predikált érték [Nm3/h]",line=dict(color='rgba(0,200,0,0.8)') ,showlegend=True  ),

        row=1, col=1

    )
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_test), name="Mért érték [Nm3/h]", line=dict(color='rgba(200,0,0,0.3)') ,showlegend=True  ),

        row=1, col=1
    )
    fig0.update_layout(
        autosize=False,
        width=1200,
        height=600,
        )

    fig0.show()

#Nitrogén predikció
!rm /content/all.csv
!wget "https://github.com/sipocz/messer/raw/main/all.csv"
!head /content/all.csv

import numpy as np
import pandas as pd
import plotly.express as px

import plotly.graph_objects as go

_PCVERSION_=False
_GITHUB_ORIG=True

if _PCVERSION_:
    basedir="C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/nitrogen"
else:

    if _GITHUB_ORIG:
        basedir="/content/"
    else:
        from google.colab import drive
        drive.mount('/content/drive',force_remount=True)
        basedir="/content/drive/My Drive/001_AI/messer/nitrogen/"
print(basedir)

df=pd.read_csv(basedir+"all.csv")
df.head()

df["timeStamp"]=pd.to_datetime(df["timeStamp"],format="%Y.%m.%d %H:%M:%S")   # konvertáljuk az időbélyeget


d=df[["timeStamp","PSA_M"]]
d.set_index("timeStamp",inplace=True)

agg_10m = d.groupby([pd.Grouper(freq='D')])
resample_mean =  d.resample("60min").agg({'PSA_M': 'mean'})  # újramintázzuk a jeleket, pl órás átlagokkal számoljunk
resample_max =  d.resample("60min").agg({'PSA_M': 'max'})    # újramintázzuk a jeleket, pl órás maximumok
resample_min =  d.resample("60min").agg({'PSA_M': 'min'})    # újramintázzuk a jeleket, pl órás minimumok
o_df=pd.DataFrame({"Max":resample_max.PSA_M,"Min":resample_min.PSA_M, "Mean":resample_mean.PSA_M})

print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
o_df.dropna(inplace=True)
print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )

from sklearn.cluster import DBSCAN
import numpy as np
X = o_df
print(type(X))
clustering = DBSCAN(eps=100, min_samples=1).fit(X)
print(clustering.labels_)

print (o_df.describe())

amax=df.max
o=[index for (index,i) in enumerate(clustering.labels_) if i>0]
print(clustering.labels_[o])
print(o_df.Max[o])

print("Ezek lesznek az Outlierek")
print(o_df.Max[o])
print("\n\nAz eredeti Dataframe:\n",o_df.describe())
o_df_clear=o_df.drop(o_df.index[o])
print("\n\nA végleges Dataframe:\n",o_df_clear.describe())

df=pd.DataFrame(clustering.labels_)

fig = px.histogram(o_df_clear,x="Max",opacity=0.8,nbins=400)
fig.show()

def create_train_data(df,selector="Mean", sample=10,test_size=0.2,printout=False,scale=False):
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import MinMaxScaler
    '''
    df dataframe selector által meghatározott oszlopából 10 sample méretű idősor tanulóset-et készít
    
    '''
    sample+=1
    X_train=[]
    y_train=[]
    X_test=[]
    y_test=[]
    
    

    train_data, test_data = train_test_split(df, test_size=test_size, shuffle=False)
    
    for t in range(len(list(train_data[selector]))-sample):
        a=list(train_data[selector][t:t+sample])
    
        #print(a)
        X_train.append(a[0:sample-1])
        #print(a)
        y_train.append([a[-1]])

    for t in range(len(list(test_data[selector]))-sample):
        a=list(test_data[selector][t:t+sample])
        X_test.append(a[0:sample-1])
        y_test.append([a[-1]])

    if printout:
        for i in range(10):
            print(X_train[i], y_train[i])

    return(X_train,y_train,X_test,y_test)



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
sample=3
testsize=0.2
scaler = MinMaxScaler(feature_range=(0,1.0))
scaler.fit(o_df_clear)
o_df_clear2=scaler.transform(o_df_clear)
o_df_norm=pd.DataFrame(data=o_df_clear2,columns=["Min","Max","Mean"])

Mean_X_train,Mean_y_train,Mean_X_test,Mean_y_test=create_train_data(o_df_norm, selector="Mean", sample=sample,test_size=testsize,printout=True)



o_df_norm

!pip install tensorflow-addons

def scaler(x):
    return(x*0.5)

from tensorflow_addons.optimizers import CyclicalLearningRate,TriangularCyclicalLearningRate
 
cyclical_learning_rate = TriangularCyclicalLearningRate(
 initial_learning_rate=0.00000001,
 maximal_learning_rate=0.000001,
 step_size=1991*5,
 
 scale_mode='cycle')

def lr_schedule():
  """
  Returns a custom learning rate that decreases as epochs progress.
  """
  learning_rate = 0.2
  if epoch > 2:
    learning_rate = 0.02
  if epoch > 3:
    learning_rate = 0.01
  if epoch > 20:
    learning_rate = 0.0000005

# Commented out IPython magic to ensure Python compatibility.
# %%html
# 
# <code>from keras.layers import InputLayer, Dense, LSTM, Input, Dropout</code><br>
# <code>from keras.models import Sequential, Model</code><br>
# <code>from keras.models import Sequential, Model</code><br>
# <code>from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta</code><br>
# <code>import tensorflow as tf</code><br>
# <code>from tensorflow.keras.callbacks import ModelCheckpoint</code><br>
# <code>from keras.backend import clear_session</code><br>
# <code>from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld</code><br>
# <code>from tensorflow_addons.optimizers import CyclicalLearningRate</code><br>
# <code>clear_session()</code><br>
# 
# 
# <code>input_size=len(Mean_X_train[0][:])</code><br>
# <code>drop_frac0=0.6</code><br>
# <code>drop_frac1=0.6</code><br>
# 
# <code>input1=Input(shape=(input_size,))</code><br>
# <code>l1_out=Dense(1370,activation="swish",kernel_initializer='lecun_normal')(input1) # 202102262002 l1_out=Dense(370,activation="sigmoid")(input1)</code><br>
# <code>l2_out=Dropout(drop_frac0)(l1_out)</code><br>
# 
# 
# <code>l3_out=Dense(1300,activation="swish",kernel_initializer='lecun_normal')(l2_out)</code><br>
# <code>l4_out=Dropout(drop_frac1)(l3_out)</code><br>
# 
# 
# 
# <code>pred=Dense(1,)(l4_out)</code><br>
# 
# <code>model = Model(inputs=input1, outputs=pred)</code><br>
# <code>optimizer=Adamax(learning_rate=cyclical_learning_rate,) #202102261859 ~0,0038: Adamax(learning_rate=0.01,) #SGD(lr=0.005, decay=1e-4, momentum=0.9)</code><br>
# 
# <code>model.compile(loss='mae',</code><br>
# <code>    optimizer=optimizer,</code><br>
# <code>    metrics=["mae"])</code><br>
# <code>

from keras.layers import InputLayer, Dense, LSTM, Input, Dropout
from keras.models import Sequential, Model
from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.backend import clear_session
from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld
from tensorflow_addons.optimizers import CyclicalLearningRate
clear_session()

kernel_reg_1=tf.keras.regularizers.L2(0.1)

input_size=len(Mean_X_train[0][:])
drop_frac0=0.6  #0.6 jó
drop_frac1=0.6  

input1=Input(shape=(input_size,))
l1_out=Dense(2600,activation="swish",kernel_initializer='lecun_normal')(input1) # 202102262002 l1_out=Dense(370,activation="sigmoid")(input1)
l2_out=Dropout(drop_frac0)(l1_out)
# 2600 as értékkel 0.033

l3_out=Dense(2000,activation="swish",kernel_initializer='lecun_normal',)(l2_out)
l4_out=Dropout(drop_frac1)(l3_out)
#2000-vel jól megy


pred=Dense(1,)(l4_out)

model = Model(inputs=input1, outputs=pred)
optimizer=Adamax(learning_rate=0.0005,) #202102261859 ~0,0038: Adamax(learning_rate=0.01,) #SGD(lr=0.005, decay=1e-4, momentum=0.9)

model.compile(loss='mae',
    optimizer=optimizer,
    metrics=["mae"])

from keras.callbacks import TensorBoard

# Black magic for getting Tboard to work with Keras on Colab...
# Don't ask! :-P
tb=TensorBoard(log_dir='./logs',)
#tbc=TensorBoardColab(graph_path='./Graph')
import datetime, os
logdir = os.path.join("./logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1,write_images=True)

print(logdir)
#file_writer = tf.summary.FileWriter(logdir, sess.graph)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

!rm model*

MONITOR = "loss" #try MONITOR = "val_sparse_categorical_accuracy" as well

rates=[
       [450,150],
    
       
       
       


]
for rate in rates:
    i=1
    mname=str(rate[0])+"_"+str(rate[1])
    history = model.fit(Mean_X_train, Mean_y_train, epochs=rate[0], batch_size=rate[1], validation_data=(Mean_X_test, Mean_y_test),
                    
                    callbacks=[tensorboard_callback,ModelCheckpoint("model_"+mname+"_{epoch}.hdf5",
                              monitor = MONITOR,
                              save_best_only = True,
                              save_weights_only = False,
                              save_freq= 1,
                              verbose = 0),],verbose=1)




ypred = model.predict(Mean_X_test)

print(history.history.keys())

!pip install -q pyyaml h5py

!model.save_weights("./20210308weight.keras")

y_pred = model.predict(Mean_X_test)
y_pred=y_pred.reshape(-1,)
y_test=[i[0] for i in Mean_y_test]

grafikon(y_pred,y_test)

y_pred = model.predict(Mean_X_train)
y_pred=y_pred.reshape(-1,)
y_test=[i[0] for i in Mean_y_train]

grafikon(y_pred,y_test)

!wget  "https://github.com/sipocz/messer/raw/main/modells/202102262202.hdf5"

!ls --full-time

model.load_weights("model_2_4_0.026917073875665665.h5")

path2='./models/202102280000.keras'
#model2 = tf.keras.models.load( path2 )
new_model = tf.keras.models.load_model(path2)

new_model.summary()

y_pred = model.predict(Mean_X_test)
y_pred=y_pred.reshape(-1,)
y_test=[i[0] for i in Mean_y_test]

grafikon(y_pred,y_test)

!wget  "https://github.com/sipocz/messer/raw/main/modells/model_22_3.2090702006826177e-06.hdf5"

!ls --full-time

new_model = tf.keras.models.load_model("/content/models/202102280034.keras")
y_pred = new_model.predict(Mean_X_test)
y_pred=y_pred.reshape(-1,)
y_test=[i[0] for i in Mean_y_test]

grafikon(y_pred,y_test)

tf.version.VERSION

model.load_weights("/content/models/a1/variables/variables.index")

"""#Becsüljünk egy várható range-et

Vizsgáljuk meg, hogy mennyire tudunk előre predikálni az elmúlt időszak adataiból, hogy a következő időszakban milyen tartományban lesznek az értékek. Ez valahol a FB Prophet-re hasonlít. A célunk az, hogy egy riasztási rendszert készítsünk ami a becsült range alapján dönt a riasztásról!
"""

def grafikon(y_pred,y_test):

    x_=[i for i in range(len(y_pred))]

    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    fig0 = make_subplots(rows=1, cols=1,)
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_pred), name="Predikált érték [Nm3/h]",line=dict(color='rgba(0,200,0,0.8)') ,showlegend=True  ),

        row=1, col=1

    )
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_test), name="Mért érték [Nm3/h]", line=dict(color='rgba(200,0,0,0.3)') ,showlegend=True  ),

        row=1, col=1
    )
    fig0.update_layout(
        autosize=False,
        width=1200,
        height=600,
        )

    fig0.show()

#Nitrogén predikció
!rm /content/all.csv
!wget "https://github.com/sipocz/messer/raw/main/all.csv"
!head /content/all.csv

_PCVERSION_=False
_GITHUB_ORIG=True

if _PCVERSION_:
    basedir="C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/nitrogen"
else:

    if _GITHUB_ORIG:
        basedir="/content/"
    else:
        from google.colab import drive
        drive.mount('/content/drive',force_remount=True)
        basedir="/content/drive/My Drive/001_AI/messer/nitrogen/"
print(basedir)

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

df=pd.read_csv(basedir+"all.csv")
df["timeStamp"]=pd.to_datetime(df["timeStamp"],format="%Y.%m.%d %H:%M:%S")   # konvertáljuk az időbélyeget
d=df[["timeStamp","PSA_M"]]

#print(d.head())
di=[]
for i in d.index:
   
    
    if d.iloc[i].PSA_M>1200:
        print(d.iloc[i].PSA_M,end=",")
        di.append(i)    

d.drop(index=di, inplace=True)

d.set_index("timeStamp",inplace=True)
#agg_10m = d.groupby([pd.Grouper(freq='D')])
resample_mean =  d.resample("15min").agg({'PSA_M': 'mean'})  # újramintázzuk a jeleket, pl órás átlagokkal számoljunk
resample_max =  d.resample("15min").agg({'PSA_M': 'max'})    # újramintázzuk a jeleket, pl órás maximumok
resample_min =  d.resample("15min").agg({'PSA_M': 'min'})    # újramintázzuk a jeleket, pl órás minimumok
o_df=pd.DataFrame({"Max":resample_max.PSA_M,"Min":resample_min.PSA_M, "Mean":resample_mean.PSA_M})


print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
o_df.dropna(inplace=True)
print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
print(o_df.head())

from sklearn.cluster import DBSCAN
import numpy as np
X = o_df
print(type(X))
clustering = DBSCAN(eps=170, min_samples=1).fit(X)
print(clustering.labels_)

print (o_df.describe())

amax=df.max
o=[index for (index,i) in enumerate(clustering.labels_) if i>0]
print(clustering.labels_[o])
print(o_df.Max[o])

print("Ezek lesznek az Outlierek")
print(o_df.Max[o])
print("\n\nAz eredeti Dataframe:\n",o_df.describe())
o_df_clear=o_df.drop(o_df.index[o])
print("\n\nA végleges Dataframe:\n",o_df_clear.describe())

o_df_clear.head()

#df=pd.DataFrame(clustering.labels_)

def dfhist(odf,col):
    fig = px.histogram(odf,x=col,opacity=0.8,nbins=400)
    fig.show()

dfhist(o_df_clear,"Min")

def create_train_data_2(df,selector="Mean", sample=10,test_size=0.2,printout=False,scale=False):
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import MinMaxScaler
    '''
    df dataframe selector által meghatározott oszlopából 10 sample méretű idősor tanulóset-et készít
    
    '''
    sample+=1
    X_train=[]
    y_train=[]
    X_test=[]
    y_test=[]
    
    

    train_data, test_data = train_test_split(df, test_size=test_size, shuffle=False)
    #print(test_data.Min)
    db=len(list(train_data[selector]))
    print(f"Train: {db}. db")
    for t in range(len(list(train_data[selector]))-sample):
        a=list(train_data[selector][t:t+sample])
        maxi=train_data["Max"][t+sample-1]
        mini=train_data["Min"][t+sample-1]

    
        #print(a)
        X_train.append(a[0:sample-1])
        #print(a)
        y_train.append(1.0*maxi)
    
    db=len(list(test_data[selector]))
    print(f"Test: {db}. db")
    for t in range(db-sample):
        #print(t)
        a=list(test_data[selector][t:t+sample])
        maxi=test_data.Max[t+sample-1]
        mini=test_data.Min[t+sample-1]
        
        X_test.append(a[0:sample-1])
        y_test.append(maxi-0)
        
    
    if printout:
        for i in range(10):
            print(X_train[i], y_train[i])

    return(X_train,y_train,X_test,y_test)

Mean_X_train,Mean_y_train,Mean_X_test,Mean_y_test=create_train_data_2(o_df_clear, selector="Mean", sample=10,test_size=0.1,printout=False)

o_df_clear.tail()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
sample=5
testsize=0.1

scaler = MinMaxScaler(feature_range=(-1.0,1.0))  # 0..1 tartománnyal egész jó
scaler.fit(o_df_clear)

o_df_clear2=scaler.transform(o_df_clear)
o_df_norm=pd.DataFrame(data=o_df_clear2,columns=["Max","Min","Mean"], index=o_df_clear.index)

df_train,df_test=train_test_split(o_df_norm,test_size=0.1,shuffle=False)

Mean_X_train,Mean_y_train,Mean_X_val,Mean_y_val=create_train_data_2(df_train, selector="Mean", sample=sample,test_size=testsize,printout=False)

Mean_X_test,Mean_y_test,_,_=create_train_data_2(df_test, selector="Mean", sample=sample,test_size=0.0001,printout=False)



!pip install tensorflow-addons

from keras.layers import InputLayer, Dense, LSTM, Input, Dropout
from keras.models import Sequential, Model
from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta,Adagrad,Nadam
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.backend import clear_session
from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld
import tensorflow_addons as tfa

clear_session()

kernel_reg_1=tf.keras.regularizers.L2(0.1)

input_size=len(Mean_X_train[0][:])
drop_frac0=0.7  #0.6 jó
drop_frac1=0.6  
'''
desc=[
        {"node":100,"activation":"swish","kernel_init":"lecun_normal"},
        {"node":120,"activation":"swish","kernel_init":"lecun_normal"},
        {"node":110,"activation":"swish","kernel_init":"lecun_normal"},
]
80X32 0.060
'''
desc=[
        {"node":200,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":220,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":210,"activation":"tanh","kernel_init":"lecun_normal"},
]

'''

desc=[
        {"node":200,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":220,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":210,"activation":"tanh","kernel_init":"lecun_normal"},
]
2x80x48: 0.0028 loss: msle learning rate=0.00005
'''

desc=[
        {"node":200,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":220,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":210,"activation":"tanh","kernel_init":"lecun_normal"},
       
      
       
      
]



input1=Input(shape=(input_size,))
layer_input=input1
for idx,_ in enumerate(desc):
    l1_out=Dense(desc[idx]["node"],activation=desc[idx]["activation"],kernel_initializer=desc[idx]["kernel_init"])(layer_input) # 202102262002 l1_out=Dense(370,activation="sigmoid")(input1)
    l2_out=Dropout(drop_frac0)(l1_out)
    layer_input=l2_out
    # 2600 as értékkel 0.033

#l3_out=Dense(2000,activation="sigmoid",kernel_initializer='lecun_normal',)(l2_out)
#l4_out=Dropout(drop_frac1)(l3_out)
#2000-vel jól megy


pred=Dense(1,)(l2_out)

model = Model(inputs=input1, outputs=pred)
optimizer=Adamax(learning_rate=0.00005,) #202102261859 ~0,0038: Adamax(learning_rate=0.01,) #SGD(lr=0.005, decay=1e-4, momentum=0.9)
optimizer=Nadam(learning_rate=0.00005,)
optimizer=tfa.optimizers.LazyAdam(0.001)
opt = tf.keras.optimizers.Adamax(learning_rate=0.00005)
optimizer = tfa.optimizers.Lookahead(opt)

model.compile(loss='mse',
    optimizer=optimizer,
    metrics=["mse"])



model.summary()

# 3*80*48

rates=[
       [50,48],
    
       
       
       


]
for rate in rates:
    i=1
    mname=str(rate[0])+"_"+str(rate[1])
    history = model.fit(Mean_X_train, Mean_y_train, 
                        epochs=rate[0], 
                        batch_size=rate[1], 
                        validation_data=(Mean_X_val, Mean_y_val),
                  
                        verbose=1
                              
                              )




ypred = model.predict(Mean_X_test)

history.history

def display_history(history):
    from matplotlib import pyplot as plt
    """Summarize history for accuracy and loss.
    """
    plt.plot(history.history['mse'])
    plt.plot(history.history['val_mse'])
    plt.title('Model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'valid'], loc='upper left')
    plt.show()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'valid'], loc='upper left')
    plt.show()
    
display_history(history)



y_pred = model.predict(Mean_X_val)
y_pred=y_pred.reshape(-1,)
y_test=[i for i in Mean_y_val]

grafikon(y_pred,y_test)

a_range=slice(2000,2500)
y_pred = model.predict(Mean_X_train[a_range])
y_pred=y_pred.reshape(-1,)
y_test=[i for i in Mean_y_train[a_range]]

grafikon(y_pred,y_test)

y_pred = model.predict(Mean_X_test)
y_pred=y_pred.reshape(-1,)
y_test=[i for i in Mean_y_test]

grafikon(y_pred,y_test)

'''
                    callbacks=[tensorboard_callback,ModelCheckpoint("model_"+mname+"_{epoch}.hdf5",
                              monitor = MONITOR,
                              save_best_only = True,
                              save_weights_only = False,
                              save_freq= 1,
                              verbose = 0),]
                              
                              
                    '''

"""# Hogy viselkedik a rendszer, ha lesz info a hétvégéről is?

"""

#Nitrogén predikció
!rm /content/all.csv
!wget "https://github.com/sipocz/messer/raw/main/all.csv"
!head /content/all.csv

_PCVERSION_=False
_GITHUB_ORIG=True

if _PCVERSION_:
    basedir="C:/Users/sipocz/OneDrive/Dokumentumok/GitHub/nitrogen"
else:

    if _GITHUB_ORIG:
        basedir="/content/"
    else:
        from google.colab import drive
        drive.mount('/content/drive',force_remount=True)
        basedir="/content/drive/My Drive/001_AI/messer/nitrogen/"
print(basedir)

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

df=pd.read_csv(basedir+"all.csv")
df["timeStamp"]=pd.to_datetime(df["timeStamp"],format="%Y.%m.%d %H:%M:%S")   # konvertáljuk az időbélyeget
d=df[["timeStamp","PSA_M"]]

#print(d.head())
di=[]
for i in d.index:
   
    
    if d.iloc[i].PSA_M>1200:
        print(d.iloc[i].PSA_M,end=",")
        di.append(i)    

d.drop(index=di, inplace=True)

d.set_index("timeStamp",inplace=True)
#agg_10m = d.groupby([pd.Grouper(freq='D')])
resample_mean =  d.resample("15min").agg({'PSA_M': 'mean'})  # újramintázzuk a jeleket, pl órás átlagokkal számoljunk
resample_max =  d.resample("15min").agg({'PSA_M': 'max'})    # újramintázzuk a jeleket, pl órás maximumok
resample_min =  d.resample("15min").agg({'PSA_M': 'min'})    # újramintázzuk a jeleket, pl órás minimumok
o_df=pd.DataFrame({"Max":resample_max.PSA_M,"Min":resample_min.PSA_M, "Mean":resample_mean.PSA_M})


print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
o_df.dropna(inplace=True)
print(f"{o_df['Min'].isnull().sum()} db NAN van a DF-ben" )
print(o_df.head())

print(o_df.index.weekday)

print(o_df.index[1000].weekday())
o_df.insert(column="date",value=o_df.index.weekday,loc=0)

o_df.head()

def unnepek(datum):
    '''
2019
Január 1-e keddi nap lesz.
Március 15. péntekre esik. (Így itt lesz egy háromnapos hétvége.)
Nagypéntek április 19-ére esik, így húsvét hétfővel (április 22-vel) együtt ott négy napot pihenhetünk.
Május 1. a munka ünnepe szerdai napra esik, így ott a hét közepén lesz egy munkaszüneti nap.
Június 10. Pünkösd hétfő. (Háromnapos hétvége.)
Augusztus 20. kedd, itt szintén négy napunk lesz egyben. (Augusztus 19-e pihenőnap.)
Október 23. szerdai napra esik.
November 1. péntek. (Háromnapos hétvége.)
December 25-26 karácsony - szerda, csütörtökre esik. (December 24-ét és 27-ét kiadják pihenőnapnak. Így itt hat nap pihenés lesz egyben.
Így 2019-ben összesen 7 hosszú hétvégénk lesz.
    '''
    dates=["2019-01-01","2019-03-15","2019-04-19","2019-04-20","2019-04-21","2019-04-22","2019-05-01","2019-06-10","2019-08-05",
           "2019-08-06","2019-08-07","2019-08-08","2019-08-09","2019-08-10","2019-08-11","2019-08-12","2019-08-13","2019-08-14",
           "2019-08-15","2019-08-16","2019-08-17","2019-08-18","2019-08-19","2019-08-20","2019-10-23","2019-11-01","2019-12-19",
           "2019-12-20","2019-12-21","2019-12-22","2019-12-23","2019-12-24","2019-12-25","2019-12-26","2019-12-27","2019-12-28",
           "2019-12-29","2019-12-30","2019-12-31"]



    '''
2020. március 15., vasárnap	Nemzeti ünnep	hétvége
2020. április 10., péntek	Nagypéntek	pihenőnap
2020. április 13., hétfő	Húsvét	pihenőnap (4 napos hétvége)
2020. május 1., péntek	Munka Ünnepe	pihenőnap (3 napos hétvége)
2020. június 1., hétfő	Pünkösd	pihenőnap (3 napos hétvége)
2020. augusztus 20., csütörtök	Államalapítás ünnepe	pihenőnap (4 napos hétvége)
2020. augusztus 21., péntek	pihenőnap	pihenőnap
2020. augusztus 29., szombat	munkanap	áthelyezett munkanap
2020. október 23., péntek	56-os Forradalom Ünnepe	pihenőnap (3 napos hétvége)
2020. november 1., vasárnap	Mindenszentek	hétvége
2020. december 12., szombat	munkanap	áthelyezett munkanap
2020. december 24., csütörtök	Szenteste	pihenőnap
2020. december 25., péntek	Karácsony	pihenőnap
2020. december 26., szombat	Karácsony	pihenőnap (4 napos hétvége)
2021. január 1., péntek	Új Év	pihenőnap (3 napos hétvége)
    '''
    dates1=["2020-01-01","2020-03-15","2020-04-10","2020-04-11","2020-04-12","2020-04-13",
           "2020-05-01","2019-06-01",
           "2020-08-03","2020-08-04","2020-08-05","2020-08-06","2020-08-07","2020-08-08","2020-08-09","2020-08-10","2020-08-11",
           "2020-08-12","2020-08-13","2020-08-14","2020-08-15","2020-08-16","2020-08-17","2020-08-18","2020-08-19","2020-08-20",
           "2020-08-21","2020-08-22","2020-08-23",
           "2020-10-23",
           "2020-11-01",
           "2020-12-19","2020-12-20","2020-12-21","2020-12-22","2020-12-23","2020-12-24","2020-12-25","2020-12-26","2020-12-27","2020-12-28",
           "2020-12-29","2020-12-30","2020-12-31"]
    alldate=dates+dates1
    
    #print(alldate)
    if datum in alldate:
        return(1)
    else:
        return(0)

unnepek("2020-02-15")

def create_train_data_3(df,selector="Mean", sample=10,test_size=0.2,printout=False,scale=False,corr=1.1):
    '''
    Ebben a setben benne lesz az is, hogy a hét melyik napján történnek a dolgok!
    '''

    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import MinMaxScaler
    '''
    df dataframe selector által meghatározott oszlopából 10 sample méretű idősor tanulóset-et készít
    
    '''
    sample+=1
    X_train=[]
    y_train=[]
    X_test=[]
    y_test=[]
    week=[0,0,0,0,0,0,0]
    

    train_data, test_data = train_test_split(df, test_size=test_size, shuffle=False)
    #print(test_data.Min)
    
    db=len(list(train_data[selector]))
    print(f"Train: {db}. db")
    for t in range(len(list(train_data[selector]))-sample):
        unnep=[0]
        daystr=str(train_data.index[t].date())
        unnepnap=unnepek(daystr)
        unnep[0]=unnepnap
        a=list(train_data[selector][t:t+sample])
        maxi=train_data["Max"][t+sample-1]
        mini=train_data["Min"][t+sample-1]
        week=[0,0,0,0,0,0,0]
        week[train_data["date"][t+sample-1]]=1
    
        #print(a)
        X_train.append(a[0:sample-1]+week+unnep)
        #print(a)
        y_train.append([corr*maxi])  #1.1 szorzó a 10%-os riszatásai limit miatt van!
    
    db=len(list(test_data[selector]))
    print(f"Test: {db}. db")
    for t in range(db-sample):
        #print(t)
        unnep=[0]
        daystr=str(train_data.index[t].date())
        unnepnap=unnepek(daystr)
        unnep[0]=unnepnap
        a=list(test_data[selector][t:t+sample])
        maxi=test_data.Max[t+sample-1]
        mini=test_data.Min[t+sample-1]
        week=[0,0,0,0,0,0,0]
        week[train_data["date"][t+sample-1]]=1
    
        X_test.append(a[0:sample-1]+week+unnep)
        y_test.append([corr*maxi])
        
    
    if printout:
        for i in range(10):
            print(X_train[i], y_train[i])

    return(X_train,y_train,X_test,y_test)

str(o_df.index[0].date())

create_train_data_3(o_df,selector="Mean",sample=4,test_size=0.1,printout=True,corr=1.1)
print(".")



o_df.describe()
o_df_clear=o_df

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
sample=3
testsize=0.1



o_df_norm=pd.DataFrame(data=o_df_clear,columns=["date","Max","Min","Mean"], index=o_df_clear.index)

o_df_norm.tail()

df_train,df_test=train_test_split(o_df_norm,test_size=0.1,shuffle=False)

Mean_X_train,Mean_y_train,Mean_X_val,Mean_y_val=create_train_data_3(df_train, selector="Mean", sample=sample,test_size=testsize,printout=False)

Mean_X_test,Mean_y_test,_,_=create_train_data_3(df_test, selector="Mean", sample=sample,test_size=0.0001,printout=False)

scaler = MinMaxScaler(feature_range=(-1,1))  

scaler.fit(Mean_X_train)
Mean_X_train_norm=scaler.transform(Mean_X_train)

scaler.fit(Mean_y_train)
Mean_y_train_norm=scaler.transform(Mean_y_train)

scaler.fit(Mean_y_train)
Mean_y_train_norm=scaler.transform(Mean_y_train)

scaler.fit(Mean_X_val)
Mean_X_val_norm=scaler.transform(Mean_X_val)

scaler.fit(Mean_y_val)
Mean_y_val_norm=scaler.transform(Mean_y_val)

scaler.fit(Mean_X_test)
Mean_X_test_norm=scaler.transform(Mean_X_test)

scaler.fit(Mean_y_test)
Mean_y_test_norm=scaler.transform(Mean_y_test)

!pip install tensorflow-addons

from keras.layers import InputLayer, Dense, LSTM, Input, Dropout
from keras.models import Sequential, Model
from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta,Adagrad,Nadam
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.backend import clear_session
from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld
import tensorflow_addons as tfa

clear_session()

kernel_reg_1=tf.keras.regularizers.L2(0.1)

input_size=len(Mean_X_train_norm[0][:])
print(f" Input size={input_size}")
drop_frac0=0.7  #0.6 jó
drop_frac1=0.6  
'''
desc=[
        {"node":100,"activation":"swish","kernel_init":"lecun_normal"},
        {"node":120,"activation":"swish","kernel_init":"lecun_normal"},
        {"node":110,"activation":"swish","kernel_init":"lecun_normal"},
]
80X32 0.060
'''
desc=[
        {"node":200,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":220,"activation":"tanh","kernel_init":"lecun_normal"},
        {"node":210,"activation":"tanh","kernel_init":"lecun_normal"},
]

'''

desc=[
        {"node":200,"activation":"tanh","kernel_init":"lecun_normal","drop_frac":0.6},
        {"node":220,"activation":"tanh","kernel_init":"lecun_normal","drop_frac":0.7},
        {"node":210,"activation":"tanh","kernel_init":"lecun_normal","drop_frac":0.7},
]
2x80x48: 0.0028 loss: msle learning rate=0.00005
'''

desc=[
        {"node":200,"activation":"sigmoid","kernel_init":"lecun_normal","drop_frac":0.65},
        {"node":220,"activation":"sigmoid","kernel_init":"lecun_normal","drop_frac":0.65},
        {"node":210,"activation":"sigmoid","kernel_init":"lecun_normal","drop_frac":0.65},
        # jó modell, 0.0229 lr=0.00005 feature range(-1,1) 
      
       
      
]



input1=Input(shape=(input_size,))
layer_input=input1
for idx,_ in enumerate(desc):
    l1_out=Dense(desc[idx]["node"],activation=desc[idx]["activation"],kernel_initializer=desc[idx]["kernel_init"])(layer_input) # 202102262002 l1_out=Dense(370,activation="sigmoid")(input1)
    l2_out=Dropout(desc[idx]["drop_frac"])(l1_out)
    layer_input=l2_out
    # 2600 as értékkel 0.033

#l3_out=Dense(2000,activation="sigmoid",kernel_initializer='lecun_normal',)(l2_out)
#l4_out=Dropout(drop_frac1)(l3_out)
#2000-vel jól megy


pred=Dense(1,)(l2_out)

model = Model(inputs=input1, outputs=pred)
optimizer=Adamax(learning_rate=0.00005,) #202102261859 ~0,0038: Adamax(learning_rate=0.01,) #SGD(lr=0.005, decay=1e-4, momentum=0.9)
optimizer=Nadam(learning_rate=0.00005,)
optimizer=tfa.optimizers.LazyAdam(0.001)
opt = tf.keras.optimizers.Adamax(learning_rate=0.00005)
optimizer = tfa.optimizers.Lookahead(opt)

model.compile(loss='mse',
    optimizer=optimizer,
    metrics=["mse"])

input_size

model.summary()
Mean_X_val_norm[0]

# 3*80*48

rates=  [
            [2000,45],
    
        ]
for rate in rates:
    i=1
    mname=str(rate[0])+"_"+str(rate[1])
    history = model.fit(Mean_X_train_norm, Mean_y_train_norm, 
                        epochs=rate[0], 
                        batch_size=rate[1], 
                        validation_data=(Mean_X_val_norm, Mean_y_val_norm),
                  
                        verbose=1
                              
                              )

def grafikon(y_pred,y_test):

    x_=[i for i in range(len(y_pred))]

    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    fig0 = make_subplots(rows=1, cols=1,)
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_pred), name="Predikált érték [Nm3/h]",line=dict(color='rgba(0,200,0,0.8)') ,showlegend=True  ),

        row=1, col=1

    )
    fig0.add_trace(
        go.Scatter(x=x_, y=list(y_test), name="Mért érték [Nm3/h]", line=dict(color='rgba(200,0,0,0.3)') ,showlegend=True  ),

        row=1, col=1
    )
    fig0.update_layout(
        autosize=False,
        width=1200,
        height=600,
        )

    fig0.show()

Mean_y_test_norm

y_pred = model.predict(Mean_X_test_norm)
y_pred=y_pred.reshape(-1,)
y_test=[i[0] for i in Mean_y_test_norm]

grafikon(y_pred,y_test)

#model.save("20210307xxxx.tf",)

#  !zip zippi -r 20210307xxxx.tf/* 


!pip list